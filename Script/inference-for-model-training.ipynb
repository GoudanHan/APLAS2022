{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:47:15.649652Z",
     "iopub.status.busy": "2022-09-06T18:47:15.64889Z",
     "iopub.status.idle": "2022-09-06T18:47:15.680697Z",
     "shell.execute_reply": "2022-09-06T18:47:15.679547Z",
     "shell.execute_reply.started": "2022-09-06T18:47:15.649547Z"
    }
   },
   "outputs": [],
   "source": [
    "# install packages that will be used in this notebook\n",
    "\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install tensorflow\n",
    "# !pip install torch\n",
    "# !pip install transformers\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:47:15.683769Z",
     "iopub.status.busy": "2022-09-06T18:47:15.683103Z",
     "iopub.status.idle": "2022-09-06T18:47:21.531359Z",
     "shell.execute_reply": "2022-09-06T18:47:21.530253Z",
     "shell.execute_reply.started": "2022-09-06T18:47:15.68373Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade tensorflow-gpu\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:47:21.533613Z",
     "iopub.status.busy": "2022-09-06T18:47:21.53293Z",
     "iopub.status.idle": "2022-09-06T18:47:25.124353Z",
     "shell.execute_reply": "2022-09-06T18:47:25.12319Z",
     "shell.execute_reply.started": "2022-09-06T18:47:21.533581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify the model name:bert-small\n"
     ]
    }
   ],
   "source": [
    "# provide model name here (bert-small, bert-medium, bert-base, ocamlbert-large, or codebert)\n",
    "\n",
    "model = input(\"Please specify the model name:\")\n",
    "model_name = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:47:25.131325Z",
     "iopub.status.busy": "2022-09-06T18:47:25.130517Z",
     "iopub.status.idle": "2022-09-06T18:47:27.895369Z",
     "shell.execute_reply": "2022-09-06T18:47:27.894087Z",
     "shell.execute_reply.started": "2022-09-06T18:47:25.131274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify the task:14to15\n"
     ]
    }
   ],
   "source": [
    "# provide task name here (14to15 or 15to14)\n",
    "\n",
    "task = input(\"Please specify the task:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:47:27.897724Z",
     "iopub.status.busy": "2022-09-06T18:47:27.897284Z",
     "iopub.status.idle": "2022-09-06T18:47:27.939834Z",
     "shell.execute_reply": "2022-09-06T18:47:27.938886Z",
     "shell.execute_reply.started": "2022-09-06T18:47:27.89768Z"
    }
   },
   "outputs": [],
   "source": [
    "# define functions used for data preprocessing\n",
    "\n",
    "def proccessing1(s, err = 'type error'):\n",
    "    sl = s.splitlines()\n",
    "    l_len = []\n",
    "    prog = ''\n",
    "    for i in sl:\n",
    "        if \"(*\" in i:\n",
    "            break\n",
    "        l_len.append(len(i))\n",
    "        prog += i\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for i in range(len(sl)):\n",
    "        if err in sl[i]:\n",
    "            start = i\n",
    "    cut = sl[start+1:]\n",
    "    for i in range(len(cut)):\n",
    "        if '*)' in cut[i]:\n",
    "            end = i\n",
    "            break\n",
    "    cut_s = cut[:end]\n",
    "    lidx = []\n",
    "    for span in cut_s:\n",
    "        sp = re.findall(r'\\d+', span)\n",
    "        span_ = [int(d) for d in sp]\n",
    "        row1, col1, row2, col2 = span_\n",
    "        s = convert_idx((row1, col1),l_len)\n",
    "        e = convert_idx((row2, col2),l_len)\n",
    "        lidx.append([s,e])\n",
    "    merged = merge_intervals(lidx)\n",
    "    for m in merged:\n",
    "        m[0] += 1\n",
    "        m[1] += 1\n",
    "        \n",
    "    return prog, merged\n",
    "    \n",
    "\n",
    "\n",
    "def proccessing2(s, err = 'changed spans'):\n",
    "    sl = s.splitlines()\n",
    "    l_len = []\n",
    "    prog = ''\n",
    "    for i in sl:\n",
    "        if \"(*\" in i:\n",
    "            break\n",
    "        l_len.append(len(i))\n",
    "        prog += i\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for i in range(len(sl)):\n",
    "        if err in sl[i]:\n",
    "            start = i\n",
    "    cut = sl[start+1:]\n",
    "    for i in range(len(cut)):\n",
    "        if '*)' in cut[i]:\n",
    "            end = i\n",
    "            break\n",
    "    cut_s = cut[:end]\n",
    "    lidx = []\n",
    "    for span in cut_s:\n",
    "        sp = re.findall(r'\\d+', span)\n",
    "        span_ = [int(d) for d in sp]\n",
    "        row1, col1, row2, col2 = span_\n",
    "        s = convert_idx((row1, col1),l_len)\n",
    "        e = convert_idx((row2, col2),l_len)\n",
    "        lidx.append([s,e])\n",
    "    merged = merge_intervals(lidx)\n",
    "    for m in merged:\n",
    "        m[0] += 1\n",
    "        m[1] += 1\n",
    "    return prog, merged\n",
    "\n",
    "def proccessing(s):\n",
    "    prog, error = proccessing1(s, err = 'type error')\n",
    "    prog, fix = proccessing2(s, err = 'changed spans')\n",
    "    return prog, error, fix\n",
    "\n",
    "\n",
    "def convert_idx(t,l):\n",
    "    r,c = t\n",
    "    idx = sum(l[:r-1]) + c-1\n",
    "    return idx\n",
    "    \n",
    "    \n",
    "def merge_intervals(temp_tuple):\n",
    "    temp_tuple.sort(key=lambda interval: interval[0])\n",
    "    merged = [temp_tuple[0]]\n",
    "    for current in temp_tuple:\n",
    "        previous = merged[-1]\n",
    "        if current[0] <= previous[1]:\n",
    "            previous[1] = max(previous[1], current[1])\n",
    "        else:\n",
    "            merged.append(current)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def merge(intervals):\n",
    "    starts = intervals[:,0]\n",
    "    ends = np.maximum.accumulate(intervals[:,1])\n",
    "    valid = np.zeros(len(intervals) + 1, dtype=np.bool)\n",
    "    valid[0] = True\n",
    "    valid[-1] = True\n",
    "    valid[1:-1] = starts[1:] >= ends[:-1]\n",
    "    return np.vstack((starts[:][valid[:-1]], ends[:][valid[1:]])).T\n",
    "\n",
    "def word_filter0(w):\n",
    "    ins = []\n",
    "    if w[-1] == 't' and w[-2] == 'e' and w[-3] == 't':\n",
    "        ins.append(w[:-3])\n",
    "        ins.append('let')\n",
    "    else:\n",
    "        ins = [w]\n",
    "    return ins\n",
    "\n",
    "def word_filter1(w):\n",
    "    # print(w)\n",
    "    ins = []\n",
    "    if len(w) >= 2:\n",
    "        if w != '':\n",
    "            if w[0] == '(':\n",
    "                ins += ['(']\n",
    "                if w[1] == '(':\n",
    "                    ins += word_filter1(w[1:])\n",
    "                else:\n",
    "                    ins += [w[1:]]\n",
    "            else:\n",
    "                ins = [w]\n",
    "        # if w[-1] == ')':\n",
    "        #     ins += [')']\n",
    "        #     if w[-2] == ')':\n",
    "        #         ins += word_filter(w[:-1])\n",
    "        #     else:\n",
    "        #         ins += [w[:-1]]\n",
    "        else:\n",
    "            ins = [w]\n",
    "    else:\n",
    "        ins = [w]\n",
    "    return ins\n",
    "\n",
    "\n",
    "def word_filter2(w):\n",
    "    def word_f2(w):\n",
    "        ins = []\n",
    "        if len(w) >= 2:\n",
    "            if w[-1] == ')':\n",
    "                ins += [')']\n",
    "                if w[-2] == ')':\n",
    "                    ins += word_f2(w[:-1])\n",
    "                else:\n",
    "                    ins += [w[:-1]]\n",
    "        else:\n",
    "            ins = [w]\n",
    "        return ins\n",
    "    if w != '':\n",
    "        if w[-1] == ')': \n",
    "            ins = word_f2(w)[::-1]\n",
    "        else:\n",
    "            ins = [w]\n",
    "    else:\n",
    "        ins = [w]\n",
    "    return ins\n",
    "\n",
    "\n",
    "def word_filter3(w):\n",
    "    ins = []\n",
    "    if w[-1] == ';' and w[-2] == ';':\n",
    "        ins.append(w[:-2])\n",
    "        ins.append(';;')\n",
    "    else:\n",
    "        ins = [w]\n",
    "    return ins\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_middle(w):\n",
    "    ins = []\n",
    "    if ',' in w:\n",
    "        i = w.index(',')\n",
    "        ins.append(w[:i])\n",
    "        ins.append(',')\n",
    "        ins.append(w[i+1:])\n",
    "        return ins\n",
    "    if '::' in w:\n",
    "        i = w.index(':')\n",
    "        ins.append(w[:i])\n",
    "        ins.append('::')\n",
    "        ins.append(w[i+2:])\n",
    "        return ins\n",
    "    if ';;' in w:\n",
    "        i = w.index(';')\n",
    "        ins.append(w[:i])\n",
    "        ins.append(';;')\n",
    "        ins.append(w[i+2:])\n",
    "        return ins\n",
    "    else:\n",
    "        return [w]\n",
    "    \n",
    "# def filter_semicolon(w):\n",
    "#     ins = []\n",
    "#     for i in range(w):\n",
    "                \n",
    "\n",
    "\n",
    "def filter0(words):\n",
    "    filtered = []\n",
    "    for w in words:\n",
    "        filtered += filter_middle(w)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def filter1(words):\n",
    "    filtered = []\n",
    "    for w in words:\n",
    "        filtered += word_filter1(w)\n",
    "    return filtered\n",
    "    \n",
    "def filter2(words):\n",
    "    filtered = []\n",
    "    for w in words:\n",
    "        filtered += word_filter2(w)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\n",
    "def filter_(words):\n",
    "    return filter2(filter1(filter0(words)))\n",
    "\n",
    "\n",
    "def get_words(prog, merged):\n",
    "    parts = []\n",
    "    tracker = []\n",
    "    sp = 0\n",
    "    for i in range(len(merged)):\n",
    "        tup = merged[i]\n",
    "        parts.append(prog[sp:tup[0]])\n",
    "        tracker.append(0)\n",
    "        parts.append(prog[tup[0]:tup[1]])\n",
    "        tracker.append(1)\n",
    "        sp = tup[1]\n",
    "    parts.append(prog[sp:])\n",
    "    tracker.append(0)\n",
    "    \n",
    "    words = []\n",
    "    label = []\n",
    "    for i in range(len(parts)):\n",
    "        # ps = filter(parts[i])\n",
    "        ps = parts[i]\n",
    "        wo = re.split('\\s+', ps)\n",
    "        # re.split('\\s+', s)\n",
    "        word = filter_(wo)\n",
    "        words += word\n",
    "        if tracker[i] == 0:\n",
    "            label += [0.0 for w in range(len(word))]\n",
    "        if tracker[i] == 1:\n",
    "            label += [1.0 for w in range(len(word))]\n",
    "            \n",
    "    \n",
    "    words_ = []\n",
    "    label_ = []        \n",
    "    for i in range(len(words)):\n",
    "        if '' != words[i]:\n",
    "            words_.append(words[i])\n",
    "            label_.append(label[i])\n",
    "            \n",
    "    return words_,label_\n",
    "\n",
    "\n",
    "def intersecc(error,fix):\n",
    "    ress = []\n",
    "    for e in error:\n",
    "        for f in fix:\n",
    "            if f[0] >= e[0] and e[1] >= f[0] and f[1] >= e[1]:\n",
    "                ress.append([f[0],e[1]])\n",
    "            elif e[0] >= f[0] and f[1] >= e[1]:\n",
    "                ress.append([e[0],e[1]])\n",
    "            elif e[0] >= f[0] and f[1] >= e[0] and e[1]>=f[1]:\n",
    "                ress.append([e[0],f[1]])\n",
    "            elif f[0] >= e[0] and f[1] <= e[1]:\n",
    "                ress.append([f[0],f[1]])\n",
    "            else:\n",
    "                pass\n",
    "    return ress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:47:27.942249Z",
     "iopub.status.busy": "2022-09-06T18:47:27.941767Z",
     "iopub.status.idle": "2022-09-06T18:47:27.950272Z",
     "shell.execute_reply": "2022-09-06T18:47:27.94928Z",
     "shell.execute_reply.started": "2022-09-06T18:47:27.942213Z"
    }
   },
   "outputs": [],
   "source": [
    "names1 = []\n",
    "train1 = []\n",
    "label1 = []\n",
    "\n",
    "train2 = []\n",
    "label2 = []\n",
    "names2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip input data files, please make sure the archive.zip is at the same directory as this notebook\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile('./archive.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:47:27.953418Z",
     "iopub.status.busy": "2022-09-06T18:47:27.951426Z",
     "iopub.status.idle": "2022-09-06T18:47:27.964749Z",
     "shell.execute_reply": "2022-09-06T18:47:27.963799Z",
     "shell.execute_reply.started": "2022-09-06T18:47:27.953305Z"
    }
   },
   "outputs": [],
   "source": [
    "# data preprocessing for running in local environment\n",
    "\n",
    "data_path = './Data'\n",
    "\n",
    "path = data_path + \"/sp14\"\n",
    "try:\n",
    "    for filename in os.listdir(path):\n",
    "        if '.ml' not in filename:\n",
    "            continue\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            s = f.read()\n",
    "            prog, error, fix = proccessing(s)\n",
    "            ins = intersecc(error,fix)\n",
    "            words_,label_ = get_words(prog, ins)\n",
    "            names1.append(filename)\n",
    "            train1.append(words_)\n",
    "            label1.append(label_)\n",
    "            f.close()\n",
    "except:\n",
    "    pass\n",
    "       \n",
    "    \n",
    "path = data_path + \"/fa15\"\n",
    "\n",
    "try:\n",
    "    for filename in os.listdir(path):\n",
    "        if '.ml' not in filename:\n",
    "            continue\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            s = f.read()\n",
    "            prog, error, fix = proccessing(s)\n",
    "            ins = intersecc(error,fix)\n",
    "            words_,label_ = get_words(prog, ins)\n",
    "            names2.append(filename)\n",
    "            train2.append(words_)\n",
    "            label2.append(label_)\n",
    "            f.close()      \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:47:27.966474Z",
     "iopub.status.busy": "2022-09-06T18:47:27.965844Z",
     "iopub.status.idle": "2022-09-06T18:47:59.44072Z",
     "shell.execute_reply": "2022-09-06T18:47:59.439685Z",
     "shell.execute_reply.started": "2022-09-06T18:47:27.966437Z"
    }
   },
   "outputs": [],
   "source": [
    "# data preprocessing for running on kaggle\n",
    "\n",
    "data_path = '../input/ocamlerrordata/Data'\n",
    "\n",
    "path = data_path + \"/sp14\"\n",
    "try:\n",
    "    for filename in os.listdir(path):\n",
    "        if '.ml' not in filename:\n",
    "            continue\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            s = f.read()\n",
    "            prog, error, fix = proccessing(s)\n",
    "            ins = intersecc(error,fix)\n",
    "            words_,label_ = get_words(prog, ins)\n",
    "            names1.append(filename)\n",
    "            train1.append(words_)\n",
    "            label1.append(label_)\n",
    "            f.close()\n",
    "except:\n",
    "    pass\n",
    "       \n",
    "    \n",
    "path = data_path + \"/fa15\"\n",
    "\n",
    "try:\n",
    "    for filename in os.listdir(path):\n",
    "        if '.ml' not in filename:\n",
    "            continue\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            s = f.read()\n",
    "            prog, error, fix = proccessing(s)\n",
    "            ins = intersecc(error,fix)\n",
    "            words_,label_ = get_words(prog, ins)\n",
    "            names2.append(filename)\n",
    "            train2.append(words_)\n",
    "            label2.append(label_)\n",
    "            f.close()      \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:47:59.442582Z",
     "iopub.status.busy": "2022-09-06T18:47:59.442219Z",
     "iopub.status.idle": "2022-09-06T18:48:00.687043Z",
     "shell.execute_reply": "2022-09-06T18:48:00.68596Z",
     "shell.execute_reply.started": "2022-09-06T18:47:59.442544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1650'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activate GPU as an accelerator\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "import sklearn\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.__version__\n",
    "\n",
    "MAX_LEN = 250\n",
    "bs = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:48:00.691074Z",
     "iopub.status.busy": "2022-09-06T18:48:00.690771Z",
     "iopub.status.idle": "2022-09-06T18:48:01.831878Z",
     "shell.execute_reply": "2022-09-06T18:48:01.830647Z",
     "shell.execute_reply.started": "2022-09-06T18:48:00.691048Z"
    }
   },
   "outputs": [],
   "source": [
    "# load tokenizer based on the model name provided above\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig, RobertaTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-large-cased', do_lower_case=False)\n",
    "\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-small', do_lower_case=False)\n",
    "\n",
    "if model == 'bert-small':\n",
    "    tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-small', do_lower_case=False)\n",
    "elif model == 'bert-base':\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "elif model == 'ocamlbert-large':\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-cased', do_lower_case=False)\n",
    "elif model == 'codebert':\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\", do_lower_case=False)\n",
    "elif model == 'bert-medium':\n",
    "    tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-medium', do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:48:01.834399Z",
     "iopub.status.busy": "2022-09-06T18:48:01.833656Z",
     "iopub.status.idle": "2022-09-06T18:48:31.920436Z",
     "shell.execute_reply": "2022-09-06T18:48:31.91934Z",
     "shell.execute_reply.started": "2022-09-06T18:48:01.834359Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize words using the tokenizer loaded above\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "\n",
    "tokenized_texts_and_labels1 = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(train1, label1)\n",
    "]\n",
    "\n",
    "tokenized_texts_and_labels2 = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(train2, label2)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "tokenized_texts1 = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels1]\n",
    "labels1 = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels1]\n",
    "\n",
    "tokenized_texts2 = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels2]\n",
    "labels2 = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels2]\n",
    "\n",
    "\n",
    "input_ids1 = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts1],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "input_ids2 = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts2],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "\n",
    "tag_values1 = list(set(label1[0]))\n",
    "tag_values1.append(\"PAD\")\n",
    "tag2idx1 = {t: i for i, t in enumerate(tag_values1)}\n",
    "\n",
    "tags1 = pad_sequences([[tag2idx1.get(l) for l in lab] for lab in labels1],\n",
    "                     maxlen=MAX_LEN, value=tag2idx1[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "\n",
    "\n",
    "tag_values2 = list(set(label2[0]))\n",
    "tag_values2.append(\"PAD\")\n",
    "tag2idx2 = {t: i for i, t in enumerate(tag_values2)}\n",
    "\n",
    "tags2 = pad_sequences([[tag2idx2.get(l) for l in lab] for lab in labels2],\n",
    "                     maxlen=MAX_LEN, value=tag2idx2[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "attention_masks1 = [[float(i != 0.0) for i in ii] for ii in input_ids1]\n",
    "\n",
    "attention_masks2 = [[float(i != 0.0) for i in ii] for ii in input_ids2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:48:31.923167Z",
     "iopub.status.busy": "2022-09-06T18:48:31.922319Z",
     "iopub.status.idle": "2022-09-06T18:48:32.006076Z",
     "shell.execute_reply": "2022-09-06T18:48:32.004998Z",
     "shell.execute_reply.started": "2022-09-06T18:48:31.923127Z"
    }
   },
   "outputs": [],
   "source": [
    "# divide train and valid datasets based on the task provided before\n",
    "\n",
    "\n",
    "if task == '14to15':\n",
    "    tr_inputs = torch.tensor(input_ids1)\n",
    "    val_inputs = torch.tensor(input_ids2)\n",
    "    tr_tags = torch.tensor(tags1)\n",
    "    val_tags = torch.tensor(tags2)\n",
    "    tr_masks = torch.tensor(attention_masks1)\n",
    "    val_masks = torch.tensor(attention_masks2)\n",
    "elif task == '15to14':\n",
    "    tr_inputs = torch.tensor(input_ids2)\n",
    "    val_inputs = torch.tensor(input_ids1)\n",
    "    tr_tags = torch.tensor(tags2)\n",
    "    val_tags = torch.tensor(tags1)\n",
    "    tr_masks = torch.tensor(attention_masks2)\n",
    "    val_masks = torch.tensor(attention_masks1)\n",
    "\n",
    "# if task == '14to14':\n",
    "#     tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids1, tags1,\n",
    "#                                                             random_state=2018, test_size=0.1)\n",
    "#     tr_masks, val_masks, _, _ = train_test_split(attention_masks1, input_ids1,\n",
    "#                                                  random_state=2018, test_size=0.1)\n",
    "#     tr_inputs = torch.tensor(tr_inputs)\n",
    "#     val_inputs = torch.tensor(val_inputs)\n",
    "#     tr_tags = torch.tensor(tr_tags)\n",
    "#     val_tags = torch.tensor(val_tags)\n",
    "#     tr_masks = torch.tensor(tr_masks)\n",
    "#     val_masks = torch.tensor(val_masks)\n",
    "# elif task == '15to15':\n",
    "#     tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids2, tags2,\n",
    "#                                                             random_state=2018, test_size=0.1)\n",
    "#     tr_masks, val_masks, _, _ = train_test_split(attention_masks2, input_ids2,\n",
    "#                                                  random_state=2018, test_size=0.1)\n",
    "#     tr_inputs = torch.tensor(tr_inputs)\n",
    "#     val_inputs = torch.tensor(val_inputs)\n",
    "#     tr_tags = torch.tensor(tr_tags)\n",
    "#     val_tags = torch.tensor(val_tags)\n",
    "#     tr_masks = torch.tensor(tr_masks)\n",
    "#     val_masks = torch.tensor(val_masks)\n",
    "# elif task == '14to15':\n",
    "#     tr_inputs = torch.tensor(input_ids1)\n",
    "#     val_inputs = torch.tensor(input_ids2)\n",
    "#     tr_tags = torch.tensor(tags1)\n",
    "#     val_tags = torch.tensor(tags2)\n",
    "#     tr_masks = torch.tensor(attention_masks1)\n",
    "#     val_masks = torch.tensor(attention_masks2)\n",
    "# elif task == '15to14':\n",
    "#     tr_inputs = torch.tensor(input_ids2)\n",
    "#     val_inputs = torch.tensor(input_ids1)\n",
    "#     tr_tags = torch.tensor(tags2)\n",
    "#     val_tags = torch.tensor(tags1)\n",
    "#     tr_masks = torch.tensor(attention_masks2)\n",
    "#     val_masks = torch.tensor(attention_masks1)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:48:32.008327Z",
     "iopub.status.busy": "2022-09-06T18:48:32.007575Z",
     "iopub.status.idle": "2022-09-06T18:48:32.015839Z",
     "shell.execute_reply": "2022-09-06T18:48:32.01471Z",
     "shell.execute_reply.started": "2022-09-06T18:48:32.008284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Task1: sp14 -> sp14\n",
    "# tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids1, tags1,\n",
    "#                                                             random_state=2018, test_size=0.1)\n",
    "# tr_masks, val_masks, _, _ = train_test_split(attention_masks1, input_ids1,\n",
    "#                                              random_state=2018, test_size=0.1)\n",
    "# tr_inputs = torch.tensor(tr_inputs)\n",
    "# val_inputs = torch.tensor(val_inputs)\n",
    "# tr_tags = torch.tensor(tr_tags)\n",
    "# val_tags = torch.tensor(val_tags)\n",
    "# tr_masks = torch.tensor(tr_masks)\n",
    "# val_masks = torch.tensor(val_masks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Task2: fa15 -> fa15\n",
    "# tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids2, tags2,\n",
    "#                                                             random_state=2018, test_size=0.1)\n",
    "# tr_masks, val_masks, _, _ = train_test_split(attention_masks2, input_ids2,\n",
    "#                                              random_state=2018, test_size=0.1)\n",
    "# tr_inputs = torch.tensor(tr_inputs)\n",
    "# val_inputs = torch.tensor(val_inputs)\n",
    "# tr_tags = torch.tensor(tr_tags)\n",
    "# val_tags = torch.tensor(val_tags)\n",
    "# tr_masks = torch.tensor(tr_masks)\n",
    "# val_masks = torch.tensor(val_masks)\n",
    "\n",
    "\n",
    "# Task3: sp14->fa15\n",
    "# tr_inputs = torch.tensor(input_ids1)\n",
    "# val_inputs = torch.tensor(input_ids2)\n",
    "# tr_tags = torch.tensor(tags1)\n",
    "# val_tags = torch.tensor(tags2)\n",
    "# tr_masks = torch.tensor(attention_masks1)\n",
    "# val_masks = torch.tensor(attention_masks2)\n",
    "\n",
    "# Task4: fa15->sp14\n",
    "# tr_inputs = torch.tensor(input_ids2)\n",
    "# val_inputs = torch.tensor(input_ids1)\n",
    "# tr_tags = torch.tensor(tags2)\n",
    "# val_tags = torch.tensor(tags1)\n",
    "# tr_masks = torch.tensor(attention_masks2)\n",
    "# val_masks = torch.tensor(attention_masks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:48:32.018488Z",
     "iopub.status.busy": "2022-09-06T18:48:32.018068Z",
     "iopub.status.idle": "2022-09-06T18:48:32.035462Z",
     "shell.execute_reply": "2022-09-06T18:48:32.034414Z",
     "shell.execute_reply.started": "2022-09-06T18:48:32.018452Z"
    }
   },
   "outputs": [],
   "source": [
    "# use dataloader to create datasets used for future model training and validating\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:48:32.039767Z",
     "iopub.status.busy": "2022-09-06T18:48:32.039057Z",
     "iopub.status.idle": "2022-09-06T18:48:42.149291Z",
     "shell.execute_reply": "2022-09-06T18:48:42.148018Z",
     "shell.execute_reply.started": "2022-09-06T18:48:32.039663Z"
    }
   },
   "outputs": [],
   "source": [
    "# load pre-trained model based on the model name provided before\n",
    "\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW, BertConfig, RobertaForTokenClassification\n",
    "\n",
    "transformers.__version__\n",
    "\n",
    "if model == 'bert-small' and task[0:2] == '14':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertsmall14',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model =='bert-small' and task[0:2] == '15':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertsmall15',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'bert-base' and task[0:2] == '14':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertbase14',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'bert-base' and task[0:2] == '15':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertbase15',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'codebert' and task[0:2] == '14':\n",
    "    model = RobertaForTokenClassification.from_pretrained(\n",
    "         \"AllenGeng/CodeBert14\",\n",
    "         num_labels=len(tag2idx1),\n",
    "         output_attentions = False,\n",
    "         output_hidden_states = False\n",
    "    )\n",
    "elif model == 'codebert' and task[0:2] == '15':\n",
    "    model = RobertaForTokenClassification.from_pretrained(\n",
    "         \"AllenGeng/CodeBert15\",\n",
    "         num_labels=len(tag2idx1),\n",
    "         output_attentions = False,\n",
    "         output_hidden_states = False\n",
    "    )\n",
    "elif model == 'bert-medium' and task[0:2] == '14':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertmedium14',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'bert-medium' and task[0:2] == '15':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertmedium15',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'ocamlbert-large' and task[0:2] == '14':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/ocamlbertlarge14',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'ocamlbert-large' and task[0:2] == '15':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/ocamlbertlarge15',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:49:05.574445Z",
     "iopub.status.busy": "2022-09-06T18:49:05.573985Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in d:\\python\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in d:\\python\\lib\\site-packages (from seqeval) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in d:\\python\\lib\\site-packages (from seqeval) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\python\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in d:\\python\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\python\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                    | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.05574308031183832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   3%|██▌                                                                         | 1/30 [00:52<25:35, 52.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.1850607566334106\n",
      "Test Acc =  tensor(0.5063)\n",
      "Average train loss: 0.05258379970841548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   7%|█████                                                                       | 2/30 [01:44<24:15, 51.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.17986488226499106\n",
      "Test Acc =  tensor(0.5009)\n",
      "Average train loss: 0.04877009946195518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   7%|█████                                                                       | 2/30 [02:30<35:06, 75.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 106>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    174\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(b_input_ids, token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    175\u001b[0m                     attention_mask\u001b[38;5;241m=\u001b[39mb_input_mask, labels\u001b[38;5;241m=\u001b[39mb_labels)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Move logits and labels to CPU\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    178\u001b[0m label_ids \u001b[38;5;241m=\u001b[39m b_labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# Calculate the accuracy for this batch of test sentences.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model training and results printing\n",
    "\n",
    "model.cuda();\n",
    "# # ========================================\n",
    "# #               Validation\n",
    "# # ========================================\n",
    "# # After the completion of each training epoch, measure our performance on\n",
    "# # our validation set.\n",
    "\n",
    "# # Put the model into evaluation mode\n",
    "# model.eval()\n",
    "# # Reset the validation loss for this epoch.\n",
    "# eval_loss, eval_accuracy = 0, 0\n",
    "# nb_eval_steps, nb_eval_examples = 0, 0\n",
    "# predictions , true_labels = [], []\n",
    "# for batch in valid_dataloader:\n",
    "#     batch = tuple(t.to(device) for t in batch)\n",
    "#     b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "#     # Telling the model not to compute or store gradients,\n",
    "#     # saving memory and speeding up validation\n",
    "#     with torch.no_grad():\n",
    "#         # Forward pass, calculate logit predictions.\n",
    "#         # This will return the logits rather than the loss because we have not provided labels.\n",
    "#         outputs = model(b_input_ids, token_type_ids=None,\n",
    "#                         attention_mask=b_input_mask, labels=b_labels)\n",
    "#     # Move logits and labels to CPU\n",
    "#     logits = outputs[1].detach().cpu().numpy()\n",
    "#     label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "#     # Calculate the accuracy for this batch of test sentences.\n",
    "#     eval_loss += outputs[0].mean().item()\n",
    "#     predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "#     true_labels.extend(label_ids)\n",
    "\n",
    "# eval_loss = eval_loss / len(valid_dataloader)\n",
    "\n",
    "# pred = np.array(predictions)\n",
    "# y = np.array(true_labels)\n",
    "# pad_mask = np.array(true_labels)\n",
    "# pad_mask[pad_mask==0] = 1\n",
    "# pad_mask[pad_mask==2] = 0\n",
    "# pred = pred & pad_mask\n",
    "# y[y==2]=0\n",
    "\n",
    "# intersection = pred & y\n",
    "# union = pred | y\n",
    "# nonzero_intersection = torch.count_nonzero(torch.from_numpy(intersection), dim=1)\n",
    "# nonzero_union = torch.count_nonzero(torch.from_numpy(union), dim=1)\n",
    "# nonzero_intersection = torch.where(nonzero_union == 0, 1, nonzero_intersection)\n",
    "# nonzero_union[nonzero_union==0] = 1\n",
    "\n",
    "\n",
    "# acc = nonzero_intersection / nonzero_union\n",
    "\n",
    "# print(\"The final result is: \" + str(torch.mean(acc).numpy()))\n",
    "\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 30\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "! pip install seqeval\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "## Store the average loss after each epoch so we can plot them.\n",
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Always clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        # This will return the loss (rather than the model output)\n",
    "        # because we have provided the `labels`.\n",
    "        b_labels = b_labels.to(torch.int64)  # This line is added since running locally with a different GPU requires it. Might need to delete if you are using a different GPU.\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        # get the loss\n",
    "        loss = outputs[0]\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        total_loss += loss.item()\n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    # Reset the validation loss for this epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients,\n",
    "        # saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            b_labels = b_labels.to(torch.int64)  # This line is added since running locally with a different GPU requires it. Might need to delete if you are using a different GPU.\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    \n",
    "#     print(predictions[:5])\n",
    "#     print(true_labels[:5])\n",
    "    \n",
    "    pred = np.array(predictions)\n",
    "    y = np.array(true_labels)\n",
    "    pad_mask = np.array(true_labels)\n",
    "    pad_mask[pad_mask==0] = 1\n",
    "    pad_mask[pad_mask==2] = 0\n",
    "    pred = pred & pad_mask\n",
    "    y[y==2]=0\n",
    "    \n",
    "#     print('pred=', pred[:5])\n",
    "#     print('y=', y[:5])\n",
    "    \n",
    "    intersection = pred & y\n",
    "    union = pred | y\n",
    "\n",
    "#     print('intersection=', intersection[:5])\n",
    "#     print('union=', union[:5])\n",
    "    \n",
    "    nonzero_intersection = torch.count_nonzero(torch.from_numpy(intersection), dim=1)\n",
    "    nonzero_union = torch.count_nonzero(torch.from_numpy(union), dim=1)\n",
    "    nonzero_intersection = torch.where(nonzero_union == 0, 1, nonzero_intersection)\n",
    "    nonzero_union[nonzero_union==0] = 1\n",
    "    \n",
    "    \n",
    "#     print('nonzero_intersection=', nonzero_intersection)\n",
    "#     print('nonzero_union=', nonzero_union)\n",
    "\n",
    "    acc = nonzero_intersection / nonzero_union\n",
    "    \n",
    "    print(\"Test Acc = \", torch.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def acc_threshold(logits, threshold):\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    prob = []\n",
    "    for i in logits:\n",
    "        logits_ = torch.tensor(i)\n",
    "        prob.append(softmax(logits_))\n",
    "    res = []\n",
    "    for p in prob:\n",
    "        gg = (p[:,1] >= threshold)*1\n",
    "        res.append(gg.tolist())\n",
    "    return res\n",
    "            \n",
    "\n",
    "\n",
    "def eval_(model,threshold):\n",
    "    model.eval()\n",
    "    # Reset the validation loss for this epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            b_labels = b_labels.to(torch.int64)  # This line is added since running locally with a different GPU requires it. Might need to delete if you are using a different GPU.\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "    #    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        predictions.extend(acc_threshold(logits, threshold))\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "#     validation_loss_values.append(eval_loss)\n",
    "#     print(\"Validation loss: {}\".format(eval_loss))\n",
    "\n",
    "    pred = np.array(predictions)\n",
    "    y = np.array(true_labels)\n",
    "    pad_mask = np.array(true_labels)\n",
    "    pad_mask[pad_mask==0] = 1\n",
    "    pad_mask[pad_mask==2] = 0\n",
    "    pred = pred & pad_mask\n",
    "    y[y==2]=0\n",
    "\n",
    "    intersection = pred & y\n",
    "    union = pred | y\n",
    "\n",
    "    nonzero_intersection = torch.count_nonzero(torch.from_numpy(intersection), dim=1)\n",
    "    nonzero_union = torch.count_nonzero(torch.from_numpy(union), dim=1)\n",
    "    nonzero_intersection = torch.where(nonzero_union == 0, 1, nonzero_intersection)\n",
    "    nonzero_union[nonzero_union==0] = 1\n",
    "    acc = nonzero_intersection / nonzero_union\n",
    "    return torch.mean(acc)\n",
    "\n",
    "\n",
    "\n",
    "thel = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "# print(thel)\n",
    "\n",
    "\n",
    "\n",
    "def eval_on_thel(model,thel):\n",
    "    ress = []\n",
    "    for i in thel:\n",
    "        ress.append(eval_(model,i))\n",
    "    return ress\n",
    "accs = eval_on_thel(model,thel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When threshold equals to 0.0, model's diagnosis accuracy evaluates to 19.233%\n",
      "When threshold equals to 0.1, model's diagnosis accuracy evaluates to 54.053%\n",
      "When threshold equals to 0.2, model's diagnosis accuracy evaluates to 54.475%\n",
      "When threshold equals to 0.3, model's diagnosis accuracy evaluates to 54.957%\n",
      "When threshold equals to 0.4, model's diagnosis accuracy evaluates to 54.072%\n",
      "When threshold equals to 0.5, model's diagnosis accuracy evaluates to 52.835%\n",
      "When threshold equals to 0.6, model's diagnosis accuracy evaluates to 51.455%\n",
      "When threshold equals to 0.7, model's diagnosis accuracy evaluates to 49.615%\n",
      "When threshold equals to 0.8, model's diagnosis accuracy evaluates to 47.053%\n",
      "When threshold equals to 0.9, model's diagnosis accuracy evaluates to 41.623%\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"When threshold equals to {}, model's diagnosis accuracy evaluates to {:.3f}%\".format(thel[i],float(accs[i])*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-06T18:48:42.184492Z",
     "iopub.status.idle": "2022-09-06T18:48:42.185252Z",
     "shell.execute_reply": "2022-09-06T18:48:42.185003Z",
     "shell.execute_reply.started": "2022-09-06T18:48:42.184974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from pickle import FALSE\n",
    "from typing import Sized\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "m_x = []\n",
    "for i in range(10):\n",
    "    m_x.append(i/10)\n",
    "    accs[i] = float(accs[i])\n",
    "\n",
    "plt.plot(m_x,accs, color=\"darkseagreen\", marker=\"o\", label=model_name)\n",
    "plt.plot(m_x,accs, marker = \"o\",  color=\"darkseagreen\")\n",
    "\n",
    "plt.title(\"Impact of type error threshold on {}'s blame accuracy\".format(model_name), pad = 15)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(m_x, m_x)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "plt.grid(False)\n",
    "plt.figure(figsize=(6,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
