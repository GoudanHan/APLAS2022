{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:56:22.982651Z",
     "iopub.status.busy": "2022-08-26T20:56:22.981768Z",
     "iopub.status.idle": "2022-08-26T20:57:19.689717Z",
     "shell.execute_reply": "2022-08-26T20:57:19.688469Z",
     "shell.execute_reply.started": "2022-08-26T20:56:22.982616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\python\\lib\\site-packages (1.21.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in d:\\python\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\python\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\python\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\python\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\python\\lib\\site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\python\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\python\\lib\\site-packages (from matplotlib) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\python\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in d:\\python\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in d:\\python\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\python\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in d:\\python\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\python\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in d:\\python\\lib\\site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: packaging in d:\\python\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\python\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\python\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\python\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\python\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in d:\\python\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\python\\lib\\site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\python\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in d:\\python\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: setuptools in d:\\python\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\python\\lib\\site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\python\\lib\\site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: numpy>=1.20 in d:\\python\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in d:\\python\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in d:\\python\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\python\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\python\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\python\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\python\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in d:\\python\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\python\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\python\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\python\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\python\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\python\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\python\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\python\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\python\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\\python\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\python\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\python\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\python\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch in d:\\python\\lib\\site-packages (1.12.1)\n",
      "Requirement already satisfied: torchvision in d:\\python\\lib\\site-packages (0.13.1+cu116)\n",
      "Requirement already satisfied: torchaudio in d:\\python\\lib\\site-packages (0.12.1+cu116)\n",
      "Requirement already satisfied: typing_extensions in d:\\python\\lib\\site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: numpy in d:\\python\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\python\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\python\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\python\\lib\\site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\python\\lib\\site-packages (4.21.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\python\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\python\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in d:\\python\\lib\\site-packages (from transformers) (0.9.1)\n",
      "Requirement already satisfied: filelock in d:\\python\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\python\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in d:\\python\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\python\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\python\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\python\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests->transformers) (3.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in d:\\python\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\python\\lib\\site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\python\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\python\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in d:\\python\\lib\\site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.14.6 in d:\\python\\lib\\site-packages (from scikit-learn->sklearn) (1.21.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# install packages that will be used in this notebook\n",
    "\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install tensorflow\n",
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install transformers\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:57:19.693091Z",
     "iopub.status.busy": "2022-08-26T20:57:19.692665Z",
     "iopub.status.idle": "2022-08-26T20:57:19.699556Z",
     "shell.execute_reply": "2022-08-26T20:57:19.698141Z",
     "shell.execute_reply.started": "2022-08-26T20:57:19.693052Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade tensorflow-gpu\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:57:19.701521Z",
     "iopub.status.busy": "2022-08-26T20:57:19.701082Z",
     "iopub.status.idle": "2022-08-26T21:05:14.827531Z",
     "shell.execute_reply": "2022-08-26T21:05:14.826259Z",
     "shell.execute_reply.started": "2022-08-26T20:57:19.701493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify the model name:bert-small\n"
     ]
    }
   ],
   "source": [
    "# provide model name here (bert-small, bert-medium, bert-base, ocamlbert-large, or codebert)\n",
    "\n",
    "model = input(\"Please specify the model name:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:14.831167Z",
     "iopub.status.busy": "2022-08-26T21:05:14.830768Z",
     "iopub.status.idle": "2022-08-26T21:05:16.817180Z",
     "shell.execute_reply": "2022-08-26T21:05:16.816257Z",
     "shell.execute_reply.started": "2022-08-26T21:05:14.831128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify the task:14to15\n"
     ]
    }
   ],
   "source": [
    "# provide task name here (14to15 or 15to14)\n",
    "\n",
    "task = input(\"Please specify the task:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:16.822878Z",
     "iopub.status.busy": "2022-08-26T21:05:16.822519Z",
     "iopub.status.idle": "2022-08-26T21:05:17.267322Z",
     "shell.execute_reply": "2022-08-26T21:05:17.266181Z",
     "shell.execute_reply.started": "2022-08-26T21:05:16.822834Z"
    }
   },
   "outputs": [],
   "source": [
    "# define functions used for data preprocessing\n",
    "\n",
    "def proccessing1(s, err = 'type error'):\n",
    "    sl = s.splitlines()\n",
    "    l_len = []\n",
    "    prog = ''\n",
    "    for i in sl:\n",
    "        if \"(*\" in i:\n",
    "            break\n",
    "        l_len.append(len(i))\n",
    "        prog += i\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for i in range(len(sl)):\n",
    "        if err in sl[i]:\n",
    "            start = i\n",
    "    cut = sl[start+1:]\n",
    "    for i in range(len(cut)):\n",
    "        if '*)' in cut[i]:\n",
    "            end = i\n",
    "            break\n",
    "    cut_s = cut[:end]\n",
    "    lidx = []\n",
    "    for span in cut_s:\n",
    "        sp = re.findall(r'\\d+', span)\n",
    "        span_ = [int(d) for d in sp]\n",
    "        row1, col1, row2, col2 = span_\n",
    "        s = convert_idx((row1, col1),l_len)\n",
    "        e = convert_idx((row2, col2),l_len)\n",
    "        lidx.append([s,e])\n",
    "    merged = merge_intervals(lidx)\n",
    "    for m in merged:\n",
    "        m[0] += 1\n",
    "        m[1] += 1\n",
    "        \n",
    "    return prog, merged\n",
    "    \n",
    "\n",
    "\n",
    "def proccessing2(s, err = 'changed spans'):\n",
    "    sl = s.splitlines()\n",
    "    l_len = []\n",
    "    prog = ''\n",
    "    for i in sl:\n",
    "        if \"(*\" in i:\n",
    "            break\n",
    "        l_len.append(len(i))\n",
    "        prog += i\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for i in range(len(sl)):\n",
    "        if err in sl[i]:\n",
    "            start = i\n",
    "    cut = sl[start+1:]\n",
    "    for i in range(len(cut)):\n",
    "        if '*)' in cut[i]:\n",
    "            end = i\n",
    "            break\n",
    "    cut_s = cut[:end]\n",
    "    lidx = []\n",
    "    for span in cut_s:\n",
    "        sp = re.findall(r'\\d+', span)\n",
    "        span_ = [int(d) for d in sp]\n",
    "        row1, col1, row2, col2 = span_\n",
    "        s = convert_idx((row1, col1),l_len)\n",
    "        e = convert_idx((row2, col2),l_len)\n",
    "        lidx.append([s,e])\n",
    "    merged = merge_intervals(lidx)\n",
    "    for m in merged:\n",
    "        m[0] += 1\n",
    "        m[1] += 1\n",
    "    return prog, merged\n",
    "\n",
    "def proccessing(s):\n",
    "    prog, error = proccessing1(s, err = 'type error')\n",
    "    prog, fix = proccessing2(s, err = 'changed spans')\n",
    "    return prog, error, fix\n",
    "\n",
    "\n",
    "def convert_idx(t,l):\n",
    "    r,c = t\n",
    "    idx = sum(l[:r-1]) + c-1\n",
    "    return idx\n",
    "    \n",
    "    \n",
    "def merge_intervals(temp_tuple):\n",
    "    temp_tuple.sort(key=lambda interval: interval[0])\n",
    "    merged = [temp_tuple[0]]\n",
    "    for current in temp_tuple:\n",
    "        previous = merged[-1]\n",
    "        if current[0] <= previous[1]:\n",
    "            previous[1] = max(previous[1], current[1])\n",
    "        else:\n",
    "            merged.append(current)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def merge(intervals):\n",
    "    starts = intervals[:,0]\n",
    "    ends = np.maximum.accumulate(intervals[:,1])\n",
    "    valid = np.zeros(len(intervals) + 1, dtype=np.bool)\n",
    "    valid[0] = True\n",
    "    valid[-1] = True\n",
    "    valid[1:-1] = starts[1:] >= ends[:-1]\n",
    "    return np.vstack((starts[:][valid[:-1]], ends[:][valid[1:]])).T\n",
    "\n",
    "def word_filter0(w):\n",
    "    ins = []\n",
    "    if w[-1] == 't' and w[-2] == 'e' and w[-3] == 't':\n",
    "        ins.append(w[:-3])\n",
    "        ins.append('let')\n",
    "    else:\n",
    "        ins = [w]\n",
    "    return ins\n",
    "\n",
    "def word_filter1(w):\n",
    "    # print(w)\n",
    "    ins = []\n",
    "    if len(w) >= 2:\n",
    "        if w != '':\n",
    "            if w[0] == '(':\n",
    "                ins += ['(']\n",
    "                if w[1] == '(':\n",
    "                    ins += word_filter1(w[1:])\n",
    "                else:\n",
    "                    ins += [w[1:]]\n",
    "            else:\n",
    "                ins = [w]\n",
    "        # if w[-1] == ')':\n",
    "        #     ins += [')']\n",
    "        #     if w[-2] == ')':\n",
    "        #         ins += word_filter(w[:-1])\n",
    "        #     else:\n",
    "        #         ins += [w[:-1]]\n",
    "        else:\n",
    "            ins = [w]\n",
    "    else:\n",
    "        ins = [w]\n",
    "    return ins\n",
    "\n",
    "\n",
    "def word_filter2(w):\n",
    "    def word_f2(w):\n",
    "        ins = []\n",
    "        if len(w) >= 2:\n",
    "            if w[-1] == ')':\n",
    "                ins += [')']\n",
    "                if w[-2] == ')':\n",
    "                    ins += word_f2(w[:-1])\n",
    "                else:\n",
    "                    ins += [w[:-1]]\n",
    "        else:\n",
    "            ins = [w]\n",
    "        return ins\n",
    "    if w != '':\n",
    "        if w[-1] == ')': \n",
    "            ins = word_f2(w)[::-1]\n",
    "        else:\n",
    "            ins = [w]\n",
    "    else:\n",
    "        ins = [w]\n",
    "    return ins\n",
    "\n",
    "\n",
    "def word_filter3(w):\n",
    "    ins = []\n",
    "    if w[-1] == ';' and w[-2] == ';':\n",
    "        ins.append(w[:-2])\n",
    "        ins.append(';;')\n",
    "    else:\n",
    "        ins = [w]\n",
    "    return ins\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_middle(w):\n",
    "    ins = []\n",
    "    if ',' in w:\n",
    "        i = w.index(',')\n",
    "        ins.append(w[:i])\n",
    "        ins.append(',')\n",
    "        ins.append(w[i+1:])\n",
    "        return ins\n",
    "    if '::' in w:\n",
    "        i = w.index(':')\n",
    "        ins.append(w[:i])\n",
    "        ins.append('::')\n",
    "        ins.append(w[i+2:])\n",
    "        return ins\n",
    "    if ';;' in w:\n",
    "        i = w.index(';')\n",
    "        ins.append(w[:i])\n",
    "        ins.append(';;')\n",
    "        ins.append(w[i+2:])\n",
    "        return ins\n",
    "    else:\n",
    "        return [w]\n",
    "    \n",
    "# def filter_semicolon(w):\n",
    "#     ins = []\n",
    "#     for i in range(w):\n",
    "                \n",
    "\n",
    "\n",
    "def filter0(words):\n",
    "    filtered = []\n",
    "    for w in words:\n",
    "        filtered += filter_middle(w)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def filter1(words):\n",
    "    filtered = []\n",
    "    for w in words:\n",
    "        filtered += word_filter1(w)\n",
    "    return filtered\n",
    "    \n",
    "def filter2(words):\n",
    "    filtered = []\n",
    "    for w in words:\n",
    "        filtered += word_filter2(w)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\n",
    "def filter_(words):\n",
    "    return filter2(filter1(filter0(words)))\n",
    "\n",
    "\n",
    "def get_words(prog, merged):\n",
    "    parts = []\n",
    "    tracker = []\n",
    "    sp = 0\n",
    "    for i in range(len(merged)):\n",
    "        tup = merged[i]\n",
    "        parts.append(prog[sp:tup[0]])\n",
    "        tracker.append(0)\n",
    "        parts.append(prog[tup[0]:tup[1]])\n",
    "        tracker.append(1)\n",
    "        sp = tup[1]\n",
    "    parts.append(prog[sp:])\n",
    "    tracker.append(0)\n",
    "    \n",
    "    words = []\n",
    "    label = []\n",
    "    for i in range(len(parts)):\n",
    "        # ps = filter(parts[i])\n",
    "        ps = parts[i]\n",
    "        wo = re.split('\\s+', ps)\n",
    "        # re.split('\\s+', s)\n",
    "        word = filter_(wo)\n",
    "        words += word\n",
    "        if tracker[i] == 0:\n",
    "            label += [0.0 for w in range(len(word))]\n",
    "        if tracker[i] == 1:\n",
    "            label += [1.0 for w in range(len(word))]\n",
    "            \n",
    "    \n",
    "    words_ = []\n",
    "    label_ = []        \n",
    "    for i in range(len(words)):\n",
    "        if '' != words[i]:\n",
    "            words_.append(words[i])\n",
    "            label_.append(label[i])\n",
    "            \n",
    "    return words_,label_\n",
    "\n",
    "\n",
    "def intersecc(error,fix):\n",
    "    ress = []\n",
    "    for e in error:\n",
    "        for f in fix:\n",
    "            if f[0] >= e[0] and e[1] >= f[0] and f[1] >= e[1]:\n",
    "                ress.append([f[0],e[1]])\n",
    "            elif e[0] >= f[0] and f[1] >= e[1]:\n",
    "                ress.append([e[0],e[1]])\n",
    "            elif e[0] >= f[0] and f[1] >= e[0] and e[1]>=f[1]:\n",
    "                ress.append([e[0],f[1]])\n",
    "            elif f[0] >= e[0] and f[1] <= e[1]:\n",
    "                ress.append([f[0],f[1]])\n",
    "            else:\n",
    "                pass\n",
    "    return ress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:17.269374Z",
     "iopub.status.busy": "2022-08-26T21:05:17.268906Z",
     "iopub.status.idle": "2022-08-26T21:05:17.308908Z",
     "shell.execute_reply": "2022-08-26T21:05:17.307921Z",
     "shell.execute_reply.started": "2022-08-26T21:05:17.269338Z"
    }
   },
   "outputs": [],
   "source": [
    "names1 = []\n",
    "train1 = []\n",
    "label1 = []\n",
    "\n",
    "train2 = []\n",
    "label2 = []\n",
    "names2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip input data files, please make sure the archive.zip is at the same directory as this notebook\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile('./archive.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:17.311554Z",
     "iopub.status.busy": "2022-08-26T21:05:17.310856Z",
     "iopub.status.idle": "2022-08-26T21:05:17.321796Z",
     "shell.execute_reply": "2022-08-26T21:05:17.320789Z",
     "shell.execute_reply.started": "2022-08-26T21:05:17.311518Z"
    }
   },
   "outputs": [],
   "source": [
    "# data preprocessing for running in local environment\n",
    "# might need to change the data_path string according to where the zipped out files are at\n",
    "\n",
    "data_path = './Data'\n",
    "\n",
    "path = data_path + \"/sp14\"\n",
    "try:\n",
    "    for filename in os.listdir(path):\n",
    "        if '.ml' not in filename:\n",
    "            continue\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            s = f.read()\n",
    "            prog, error, fix = proccessing(s)\n",
    "            ins = intersecc(error,fix)\n",
    "            words_,label_ = get_words(prog, ins)\n",
    "            names1.append(filename)\n",
    "            train1.append(words_)\n",
    "            label1.append(label_)\n",
    "            f.close()\n",
    "except:\n",
    "    pass\n",
    "       \n",
    "    \n",
    "path = data_path + \"/fa15\"\n",
    "\n",
    "try:\n",
    "    for filename in os.listdir(path):\n",
    "        if '.ml' not in filename:\n",
    "            continue\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            s = f.read()\n",
    "            prog, error, fix = proccessing(s)\n",
    "            ins = intersecc(error,fix)\n",
    "            words_,label_ = get_words(prog, ins)\n",
    "            names2.append(filename)\n",
    "            train2.append(words_)\n",
    "            label2.append(label_)\n",
    "            f.close()      \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:17.324117Z",
     "iopub.status.busy": "2022-08-26T21:05:17.323450Z",
     "iopub.status.idle": "2022-08-26T21:05:22.378363Z",
     "shell.execute_reply": "2022-08-26T21:05:22.377370Z",
     "shell.execute_reply.started": "2022-08-26T21:05:17.324081Z"
    }
   },
   "outputs": [],
   "source": [
    "# data preprocessing for running on kaggle\n",
    "\n",
    "data_path = '../input/ocamlerrordata/Data'\n",
    "\n",
    "path = data_path + \"/sp14\"\n",
    "try:\n",
    "    for filename in os.listdir(path):\n",
    "        if '.ml' not in filename:\n",
    "            continue\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            s = f.read()\n",
    "            prog, error, fix = proccessing(s)\n",
    "            ins = intersecc(error,fix)\n",
    "            words_,label_ = get_words(prog, ins)\n",
    "            names1.append(filename)\n",
    "            train1.append(words_)\n",
    "            label1.append(label_)\n",
    "            f.close()\n",
    "except:\n",
    "    pass\n",
    "       \n",
    "    \n",
    "path = data_path + \"/fa15\"\n",
    "\n",
    "try:\n",
    "    for filename in os.listdir(path):\n",
    "        if '.ml' not in filename:\n",
    "            continue\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            s = f.read()\n",
    "            prog, error, fix = proccessing(s)\n",
    "            ins = intersecc(error,fix)\n",
    "            words_,label_ = get_words(prog, ins)\n",
    "            names2.append(filename)\n",
    "            train2.append(words_)\n",
    "            label2.append(label_)\n",
    "            f.close()      \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:22.380363Z",
     "iopub.status.busy": "2022-08-26T21:05:22.379989Z",
     "iopub.status.idle": "2022-08-26T21:05:22.390779Z",
     "shell.execute_reply": "2022-08-26T21:05:22.389652Z",
     "shell.execute_reply.started": "2022-08-26T21:05:22.380327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1650'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activate GPU as an accelerator\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "import sklearn\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.__version__\n",
    "\n",
    "MAX_LEN = 250\n",
    "bs = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:22.395814Z",
     "iopub.status.busy": "2022-08-26T21:05:22.395096Z",
     "iopub.status.idle": "2022-08-26T21:05:26.570120Z",
     "shell.execute_reply": "2022-08-26T21:05:26.569156Z",
     "shell.execute_reply.started": "2022-08-26T21:05:22.395776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-small\n",
      "PreTrainedTokenizer(name_or_path='prajjwal1/bert-small', vocab_size=30522, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer based on the model name provided above\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig, RobertaTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-large-cased', do_lower_case=False)\n",
    "\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-small', do_lower_case=False)\n",
    "\n",
    "if model == 'bert-small':\n",
    "    tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-small', do_lower_case=False)\n",
    "elif model == 'bert-base':\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "elif model == 'ocamlbert-large':\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-cased', do_lower_case=False)\n",
    "elif model == 'codebert':\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\", do_lower_case=False)\n",
    "elif model == 'bert-medium':\n",
    "    tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-medium', do_lower_case=False)\n",
    "    \n",
    "print(model)\n",
    "print(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:26.572530Z",
     "iopub.status.busy": "2022-08-26T21:05:26.571632Z",
     "iopub.status.idle": "2022-08-26T21:05:55.544068Z",
     "shell.execute_reply": "2022-08-26T21:05:55.543030Z",
     "shell.execute_reply.started": "2022-08-26T21:05:26.572491Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize words using the tokenizer loaded above\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "\n",
    "tokenized_texts_and_labels1 = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(train1, label1)\n",
    "]\n",
    "\n",
    "tokenized_texts_and_labels2 = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(train2, label2)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenized_texts1 = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels1]\n",
    "labels1 = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels1]\n",
    "\n",
    "tokenized_texts2 = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels2]\n",
    "labels2 = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels2]\n",
    "\n",
    "\n",
    "input_ids1 = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts1],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "input_ids2 = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts2],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "\n",
    "tag_values1 = list(set(label1[0]))\n",
    "tag_values1.append(\"PAD\")\n",
    "tag2idx1 = {t: i for i, t in enumerate(tag_values1)}\n",
    "\n",
    "tags1 = pad_sequences([[tag2idx1.get(l) for l in lab] for lab in labels1],\n",
    "                     maxlen=MAX_LEN, value=tag2idx1[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "\n",
    "\n",
    "tag_values2 = list(set(label2[0]))\n",
    "tag_values2.append(\"PAD\")\n",
    "tag2idx2 = {t: i for i, t in enumerate(tag_values2)}\n",
    "\n",
    "tags2 = pad_sequences([[tag2idx2.get(l) for l in lab] for lab in labels2],\n",
    "                     maxlen=MAX_LEN, value=tag2idx2[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "attention_masks1 = [[float(i != 0.0) for i in ii] for ii in input_ids1]\n",
    "\n",
    "attention_masks2 = [[float(i != 0.0) for i in ii] for ii in input_ids2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:55.546235Z",
     "iopub.status.busy": "2022-08-26T21:05:55.545825Z",
     "iopub.status.idle": "2022-08-26T21:05:55.620796Z",
     "shell.execute_reply": "2022-08-26T21:05:55.619831Z",
     "shell.execute_reply.started": "2022-08-26T21:05:55.546199Z"
    }
   },
   "outputs": [],
   "source": [
    "# divide train and valid datasets based on the task provided before\n",
    "\n",
    "\n",
    "if task == '14to15':\n",
    "    tr_inputs = torch.tensor(input_ids1)\n",
    "    val_inputs = torch.tensor(input_ids2)\n",
    "    tr_tags = torch.tensor(tags1)\n",
    "    val_tags = torch.tensor(tags2)\n",
    "    tr_masks = torch.tensor(attention_masks1)\n",
    "    val_masks = torch.tensor(attention_masks2)\n",
    "elif task == '15to14':\n",
    "    tr_inputs = torch.tensor(input_ids2)\n",
    "    val_inputs = torch.tensor(input_ids1)\n",
    "    tr_tags = torch.tensor(tags2)\n",
    "    val_tags = torch.tensor(tags1)\n",
    "    tr_masks = torch.tensor(attention_masks2)\n",
    "    val_masks = torch.tensor(attention_masks1)\n",
    "\n",
    "# if task == '14to14':\n",
    "#     tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids1, tags1,\n",
    "#                                                             random_state=2018, test_size=0.1)\n",
    "#     tr_masks, val_masks, _, _ = train_test_split(attention_masks1, input_ids1,\n",
    "#                                                  random_state=2018, test_size=0.1)\n",
    "#     tr_inputs = torch.tensor(tr_inputs)\n",
    "#     val_inputs = torch.tensor(val_inputs)\n",
    "#     tr_tags = torch.tensor(tr_tags)\n",
    "#     val_tags = torch.tensor(val_tags)\n",
    "#     tr_masks = torch.tensor(tr_masks)\n",
    "#     val_masks = torch.tensor(val_masks)\n",
    "# elif task == '15to15':\n",
    "#     tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids2, tags2,\n",
    "#                                                             random_state=2018, test_size=0.1)\n",
    "#     tr_masks, val_masks, _, _ = train_test_split(attention_masks2, input_ids2,\n",
    "#                                                  random_state=2018, test_size=0.1)\n",
    "#     tr_inputs = torch.tensor(tr_inputs)\n",
    "#     val_inputs = torch.tensor(val_inputs)\n",
    "#     tr_tags = torch.tensor(tr_tags)\n",
    "#     val_tags = torch.tensor(val_tags)\n",
    "#     tr_masks = torch.tensor(tr_masks)\n",
    "#     val_masks = torch.tensor(val_masks)\n",
    "# elif task == '14to15':\n",
    "#     tr_inputs = torch.tensor(input_ids1)\n",
    "#     val_inputs = torch.tensor(input_ids2)\n",
    "#     tr_tags = torch.tensor(tags1)\n",
    "#     val_tags = torch.tensor(tags2)\n",
    "#     tr_masks = torch.tensor(attention_masks1)\n",
    "#     val_masks = torch.tensor(attention_masks2)\n",
    "# elif task == '15to14':\n",
    "#     tr_inputs = torch.tensor(input_ids2)\n",
    "#     val_inputs = torch.tensor(input_ids1)\n",
    "#     tr_tags = torch.tensor(tags2)\n",
    "#     val_tags = torch.tensor(tags1)\n",
    "#     tr_masks = torch.tensor(attention_masks2)\n",
    "#     val_masks = torch.tensor(attention_masks1)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:55.622810Z",
     "iopub.status.busy": "2022-08-26T21:05:55.622426Z",
     "iopub.status.idle": "2022-08-26T21:05:55.628713Z",
     "shell.execute_reply": "2022-08-26T21:05:55.627473Z",
     "shell.execute_reply.started": "2022-08-26T21:05:55.622769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Task1: sp14 -> sp14\n",
    "# tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids1, tags1,\n",
    "#                                                             random_state=2018, test_size=0.1)\n",
    "# tr_masks, val_masks, _, _ = train_test_split(attention_masks1, input_ids1,\n",
    "#                                              random_state=2018, test_size=0.1)\n",
    "# tr_inputs = torch.tensor(tr_inputs)\n",
    "# val_inputs = torch.tensor(val_inputs)\n",
    "# tr_tags = torch.tensor(tr_tags)\n",
    "# val_tags = torch.tensor(val_tags)\n",
    "# tr_masks = torch.tensor(tr_masks)\n",
    "# val_masks = torch.tensor(val_masks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Task2: fa15 -> fa15\n",
    "# tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids2, tags2,\n",
    "#                                                             random_state=2018, test_size=0.1)\n",
    "# tr_masks, val_masks, _, _ = train_test_split(attention_masks2, input_ids2,\n",
    "#                                              random_state=2018, test_size=0.1)\n",
    "# tr_inputs = torch.tensor(tr_inputs)\n",
    "# val_inputs = torch.tensor(val_inputs)\n",
    "# tr_tags = torch.tensor(tr_tags)\n",
    "# val_tags = torch.tensor(val_tags)\n",
    "# tr_masks = torch.tensor(tr_masks)\n",
    "# val_masks = torch.tensor(val_masks)\n",
    "\n",
    "\n",
    "# Task3: sp14->fa15\n",
    "# tr_inputs = torch.tensor(input_ids1)\n",
    "# val_inputs = torch.tensor(input_ids2)\n",
    "# tr_tags = torch.tensor(tags1)\n",
    "# val_tags = torch.tensor(tags2)\n",
    "# tr_masks = torch.tensor(attention_masks1)\n",
    "# val_masks = torch.tensor(attention_masks2)\n",
    "\n",
    "# Task4: fa15->sp14\n",
    "# tr_inputs = torch.tensor(input_ids2)\n",
    "# val_inputs = torch.tensor(input_ids1)\n",
    "# tr_tags = torch.tensor(tags2)\n",
    "# val_tags = torch.tensor(tags1)\n",
    "# tr_masks = torch.tensor(attention_masks2)\n",
    "# val_masks = torch.tensor(attention_masks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:55.631259Z",
     "iopub.status.busy": "2022-08-26T21:05:55.630585Z",
     "iopub.status.idle": "2022-08-26T21:05:55.643002Z",
     "shell.execute_reply": "2022-08-26T21:05:55.641902Z",
     "shell.execute_reply.started": "2022-08-26T21:05:55.631161Z"
    }
   },
   "outputs": [],
   "source": [
    "# use dataloader to create datasets used for future model training and validating\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:55.645180Z",
     "iopub.status.busy": "2022-08-26T21:05:55.644565Z",
     "iopub.status.idle": "2022-08-26T21:06:38.106798Z",
     "shell.execute_reply": "2022-08-26T21:06:38.105751Z",
     "shell.execute_reply.started": "2022-08-26T21:05:55.645140Z"
    }
   },
   "outputs": [],
   "source": [
    "# load pre-trained model based on the model name provided before\n",
    "\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW, BertConfig, RobertaForTokenClassification\n",
    "\n",
    "transformers.__version__\n",
    "\n",
    "if model == 'bert-small' and task[0:2] == '14':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertsmall14',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model =='bert-small' and task[0:2] == '15':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertsmall15',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'bert-base' and task[0:2] == '14':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertbase14',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'bert-base' and task[0:2] == '15':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertbase15',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'codebert' and task[0:2] == '14':\n",
    "    model = RobertaForTokenClassification.from_pretrained(\n",
    "         \"AllenGeng/CodeBert14\",\n",
    "         num_labels=len(tag2idx1),\n",
    "         output_attentions = False,\n",
    "         output_hidden_states = False\n",
    "    )\n",
    "elif model == 'codebert' and task[0:2] == '15':\n",
    "    model = RobertaForTokenClassification.from_pretrained(\n",
    "         \"AllenGeng/CodeBert15\",\n",
    "         num_labels=len(tag2idx1),\n",
    "         output_attentions = False,\n",
    "         output_hidden_states = False\n",
    "    )\n",
    "elif model == 'bert-medium' and task[0:2] == '14':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertmedium14',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'bert-medium' and task[0:2] == '15':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertmedium15',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'ocamlbert-large' and task[0:2] == '14':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/ocamlbertlarge14',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'ocamlbert-large' and task[0:2] == '15':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/ocamlbertlarge15',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:06:38.108697Z",
     "iopub.status.busy": "2022-08-26T21:06:38.108336Z",
     "iopub.status.idle": "2022-08-26T21:06:58.294464Z",
     "shell.execute_reply": "2022-08-26T21:06:58.293376Z",
     "shell.execute_reply.started": "2022-08-26T21:06:38.108662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final result is: 0.52288455\n"
     ]
    }
   ],
   "source": [
    "# model training and results printing\n",
    "\n",
    "model.cuda();\n",
    "# ========================================\n",
    "#               Validation\n",
    "# ========================================\n",
    "# After the completion of each training epoch, measure our performance on\n",
    "# our validation set.\n",
    "\n",
    "# Put the model into evaluation mode\n",
    "model.eval()\n",
    "# Reset the validation loss for this epoch.\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "predictions , true_labels = [], []\n",
    "for batch in valid_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients,\n",
    "    # saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "        b_labels = b_labels.to(torch.int64)\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # This will return the logits rather than the loss because we have not provided labels.\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "    # Move logits and labels to CPU\n",
    "    logits = outputs[1].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Calculate the accuracy for this batch of test sentences.\n",
    "    eval_loss += outputs[0].mean().item()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.extend(label_ids)\n",
    "\n",
    "eval_loss = eval_loss / len(valid_dataloader)\n",
    "\n",
    "pred = np.array(predictions)\n",
    "y = np.array(true_labels)\n",
    "pad_mask = np.array(true_labels)\n",
    "pad_mask[pad_mask==0] = 1\n",
    "pad_mask[pad_mask==2] = 0\n",
    "pred = pred & pad_mask\n",
    "y[y==2]=0\n",
    "\n",
    "intersection = pred & y\n",
    "union = pred | y\n",
    "nonzero_intersection = torch.count_nonzero(torch.from_numpy(intersection), dim=1)\n",
    "nonzero_union = torch.count_nonzero(torch.from_numpy(union), dim=1)\n",
    "nonzero_intersection = torch.where(nonzero_union == 0, 1, nonzero_intersection)\n",
    "nonzero_union[nonzero_union==0] = 1\n",
    "\n",
    "\n",
    "acc = nonzero_intersection / nonzero_union\n",
    "\n",
    "print(\"The final result is: \" + str(torch.mean(acc).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
