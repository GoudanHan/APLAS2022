{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:56:22.982651Z",
     "iopub.status.busy": "2022-08-26T20:56:22.981768Z",
     "iopub.status.idle": "2022-08-26T20:57:19.689717Z",
     "shell.execute_reply": "2022-08-26T20:57:19.688469Z",
     "shell.execute_reply.started": "2022-08-26T20:56:22.982616Z"
    }
   },
   "outputs": [],
   "source": [
    "# # install packages that will be used in this notebook\n",
    "\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install tensorflow\n",
    "# !pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "# !pip install transformers\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:57:19.693091Z",
     "iopub.status.busy": "2022-08-26T20:57:19.692665Z",
     "iopub.status.idle": "2022-08-26T20:57:19.699556Z",
     "shell.execute_reply": "2022-08-26T20:57:19.698141Z",
     "shell.execute_reply.started": "2022-08-26T20:57:19.693052Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade tensorflow-gpu\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T20:57:19.701521Z",
     "iopub.status.busy": "2022-08-26T20:57:19.701082Z",
     "iopub.status.idle": "2022-08-26T21:05:14.827531Z",
     "shell.execute_reply": "2022-08-26T21:05:14.826259Z",
     "shell.execute_reply.started": "2022-08-26T20:57:19.701493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify the model name:bert-small\n"
     ]
    }
   ],
   "source": [
    "# provide model name here (bert-small, bert-medium, bert-base, ocamlbert-large, or codebert)\n",
    "\n",
    "model = input(\"Please specify the model name:\")\n",
    "model_name = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:14.831167Z",
     "iopub.status.busy": "2022-08-26T21:05:14.830768Z",
     "iopub.status.idle": "2022-08-26T21:05:16.817180Z",
     "shell.execute_reply": "2022-08-26T21:05:16.816257Z",
     "shell.execute_reply.started": "2022-08-26T21:05:14.831128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify the task:14to15\n"
     ]
    }
   ],
   "source": [
    "# provide task name here (14to15 or 15to14)\n",
    "\n",
    "task = input(\"Please specify the task:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:16.822878Z",
     "iopub.status.busy": "2022-08-26T21:05:16.822519Z",
     "iopub.status.idle": "2022-08-26T21:05:17.267322Z",
     "shell.execute_reply": "2022-08-26T21:05:17.266181Z",
     "shell.execute_reply.started": "2022-08-26T21:05:16.822834Z"
    }
   },
   "outputs": [],
   "source": [
    "# define functions used for data preprocessing\n",
    "\n",
    "def proccessing1(s, err = 'type error'):\n",
    "    sl = s.splitlines()\n",
    "    l_len = []\n",
    "    prog = ''\n",
    "    for i in sl:\n",
    "        if \"(*\" in i:\n",
    "            break\n",
    "        l_len.append(len(i))\n",
    "        prog += i\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for i in range(len(sl)):\n",
    "        if err in sl[i]:\n",
    "            start = i\n",
    "    cut = sl[start+1:]\n",
    "    for i in range(len(cut)):\n",
    "        if '*)' in cut[i]:\n",
    "            end = i\n",
    "            break\n",
    "    cut_s = cut[:end]\n",
    "    lidx = []\n",
    "    for span in cut_s:\n",
    "        sp = re.findall(r'\\d+', span)\n",
    "        span_ = [int(d) for d in sp]\n",
    "        row1, col1, row2, col2 = span_\n",
    "        s = convert_idx((row1, col1),l_len)\n",
    "        e = convert_idx((row2, col2),l_len)\n",
    "        lidx.append([s,e])\n",
    "    merged = merge_intervals(lidx)\n",
    "    for m in merged:\n",
    "        m[0] += 1\n",
    "        m[1] += 1\n",
    "        \n",
    "    return prog, merged\n",
    "    \n",
    "\n",
    "\n",
    "def proccessing2(s, err = 'changed spans'):\n",
    "    sl = s.splitlines()\n",
    "    l_len = []\n",
    "    prog = ''\n",
    "    for i in sl:\n",
    "        if \"(*\" in i:\n",
    "            break\n",
    "        l_len.append(len(i))\n",
    "        prog += i\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for i in range(len(sl)):\n",
    "        if err in sl[i]:\n",
    "            start = i\n",
    "    cut = sl[start+1:]\n",
    "    for i in range(len(cut)):\n",
    "        if '*)' in cut[i]:\n",
    "            end = i\n",
    "            break\n",
    "    cut_s = cut[:end]\n",
    "    lidx = []\n",
    "    for span in cut_s:\n",
    "        sp = re.findall(r'\\d+', span)\n",
    "        span_ = [int(d) for d in sp]\n",
    "        row1, col1, row2, col2 = span_\n",
    "        s = convert_idx((row1, col1),l_len)\n",
    "        e = convert_idx((row2, col2),l_len)\n",
    "        lidx.append([s,e])\n",
    "    merged = merge_intervals(lidx)\n",
    "    for m in merged:\n",
    "        m[0] += 1\n",
    "        m[1] += 1\n",
    "    return prog, merged\n",
    "\n",
    "def proccessing(s):\n",
    "    prog, error = proccessing1(s, err = 'type error')\n",
    "    prog, fix = proccessing2(s, err = 'changed spans')\n",
    "    return prog, error, fix\n",
    "\n",
    "\n",
    "def convert_idx(t,l):\n",
    "    r,c = t\n",
    "    idx = sum(l[:r-1]) + c-1\n",
    "    return idx\n",
    "    \n",
    "    \n",
    "def merge_intervals(temp_tuple):\n",
    "    temp_tuple.sort(key=lambda interval: interval[0])\n",
    "    merged = [temp_tuple[0]]\n",
    "    for current in temp_tuple:\n",
    "        previous = merged[-1]\n",
    "        if current[0] <= previous[1]:\n",
    "            previous[1] = max(previous[1], current[1])\n",
    "        else:\n",
    "            merged.append(current)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def merge(intervals):\n",
    "    starts = intervals[:,0]\n",
    "    ends = np.maximum.accumulate(intervals[:,1])\n",
    "    valid = np.zeros(len(intervals) + 1, dtype=np.bool)\n",
    "    valid[0] = True\n",
    "    valid[-1] = True\n",
    "    valid[1:-1] = starts[1:] >= ends[:-1]\n",
    "    return np.vstack((starts[:][valid[:-1]], ends[:][valid[1:]])).T\n",
    "\n",
    "def word_filter0(w):\n",
    "    ins = []\n",
    "    if w[-1] == 't' and w[-2] == 'e' and w[-3] == 't':\n",
    "        ins.append(w[:-3])\n",
    "        ins.append('let')\n",
    "    else:\n",
    "        ins = [w]\n",
    "    return ins\n",
    "\n",
    "def word_filter1(w):\n",
    "    # print(w)\n",
    "    ins = []\n",
    "    if len(w) >= 2:\n",
    "        if w != '':\n",
    "            if w[0] == '(':\n",
    "                ins += ['(']\n",
    "                if w[1] == '(':\n",
    "                    ins += word_filter1(w[1:])\n",
    "                else:\n",
    "                    ins += [w[1:]]\n",
    "            else:\n",
    "                ins = [w]\n",
    "        # if w[-1] == ')':\n",
    "        #     ins += [')']\n",
    "        #     if w[-2] == ')':\n",
    "        #         ins += word_filter(w[:-1])\n",
    "        #     else:\n",
    "        #         ins += [w[:-1]]\n",
    "        else:\n",
    "            ins = [w]\n",
    "    else:\n",
    "        ins = [w]\n",
    "    return ins\n",
    "\n",
    "\n",
    "def word_filter2(w):\n",
    "    def word_f2(w):\n",
    "        ins = []\n",
    "        if len(w) >= 2:\n",
    "            if w[-1] == ')':\n",
    "                ins += [')']\n",
    "                if w[-2] == ')':\n",
    "                    ins += word_f2(w[:-1])\n",
    "                else:\n",
    "                    ins += [w[:-1]]\n",
    "        else:\n",
    "            ins = [w]\n",
    "        return ins\n",
    "    if w != '':\n",
    "        if w[-1] == ')': \n",
    "            ins = word_f2(w)[::-1]\n",
    "        else:\n",
    "            ins = [w]\n",
    "    else:\n",
    "        ins = [w]\n",
    "    return ins\n",
    "\n",
    "\n",
    "def word_filter3(w):\n",
    "    ins = []\n",
    "    if w[-1] == ';' and w[-2] == ';':\n",
    "        ins.append(w[:-2])\n",
    "        ins.append(';;')\n",
    "    else:\n",
    "        ins = [w]\n",
    "    return ins\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_middle(w):\n",
    "    ins = []\n",
    "    if ',' in w:\n",
    "        i = w.index(',')\n",
    "        ins.append(w[:i])\n",
    "        ins.append(',')\n",
    "        ins.append(w[i+1:])\n",
    "        return ins\n",
    "    if '::' in w:\n",
    "        i = w.index(':')\n",
    "        ins.append(w[:i])\n",
    "        ins.append('::')\n",
    "        ins.append(w[i+2:])\n",
    "        return ins\n",
    "    if ';;' in w:\n",
    "        i = w.index(';')\n",
    "        ins.append(w[:i])\n",
    "        ins.append(';;')\n",
    "        ins.append(w[i+2:])\n",
    "        return ins\n",
    "    else:\n",
    "        return [w]\n",
    "    \n",
    "# def filter_semicolon(w):\n",
    "#     ins = []\n",
    "#     for i in range(w):\n",
    "                \n",
    "\n",
    "\n",
    "def filter0(words):\n",
    "    filtered = []\n",
    "    for w in words:\n",
    "        filtered += filter_middle(w)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def filter1(words):\n",
    "    filtered = []\n",
    "    for w in words:\n",
    "        filtered += word_filter1(w)\n",
    "    return filtered\n",
    "    \n",
    "def filter2(words):\n",
    "    filtered = []\n",
    "    for w in words:\n",
    "        filtered += word_filter2(w)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\n",
    "def filter_(words):\n",
    "    return filter2(filter1(filter0(words)))\n",
    "\n",
    "\n",
    "def get_words(prog, merged):\n",
    "    parts = []\n",
    "    tracker = []\n",
    "    sp = 0\n",
    "    for i in range(len(merged)):\n",
    "        tup = merged[i]\n",
    "        parts.append(prog[sp:tup[0]])\n",
    "        tracker.append(0)\n",
    "        parts.append(prog[tup[0]:tup[1]])\n",
    "        tracker.append(1)\n",
    "        sp = tup[1]\n",
    "    parts.append(prog[sp:])\n",
    "    tracker.append(0)\n",
    "    \n",
    "    words = []\n",
    "    label = []\n",
    "    for i in range(len(parts)):\n",
    "        # ps = filter(parts[i])\n",
    "        ps = parts[i]\n",
    "        wo = re.split('\\s+', ps)\n",
    "        # re.split('\\s+', s)\n",
    "        word = filter_(wo)\n",
    "        words += word\n",
    "        if tracker[i] == 0:\n",
    "            label += [0.0 for w in range(len(word))]\n",
    "        if tracker[i] == 1:\n",
    "            label += [1.0 for w in range(len(word))]\n",
    "            \n",
    "    \n",
    "    words_ = []\n",
    "    label_ = []        \n",
    "    for i in range(len(words)):\n",
    "        if '' != words[i]:\n",
    "            words_.append(words[i])\n",
    "            label_.append(label[i])\n",
    "            \n",
    "    return words_,label_\n",
    "\n",
    "\n",
    "def intersecc(error,fix):\n",
    "    ress = []\n",
    "    for e in error:\n",
    "        for f in fix:\n",
    "            if f[0] >= e[0] and e[1] >= f[0] and f[1] >= e[1]:\n",
    "                ress.append([f[0],e[1]])\n",
    "            elif e[0] >= f[0] and f[1] >= e[1]:\n",
    "                ress.append([e[0],e[1]])\n",
    "            elif e[0] >= f[0] and f[1] >= e[0] and e[1]>=f[1]:\n",
    "                ress.append([e[0],f[1]])\n",
    "            elif f[0] >= e[0] and f[1] <= e[1]:\n",
    "                ress.append([f[0],f[1]])\n",
    "            else:\n",
    "                pass\n",
    "    return ress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:17.269374Z",
     "iopub.status.busy": "2022-08-26T21:05:17.268906Z",
     "iopub.status.idle": "2022-08-26T21:05:17.308908Z",
     "shell.execute_reply": "2022-08-26T21:05:17.307921Z",
     "shell.execute_reply.started": "2022-08-26T21:05:17.269338Z"
    }
   },
   "outputs": [],
   "source": [
    "names1 = []\n",
    "train1 = []\n",
    "label1 = []\n",
    "\n",
    "train2 = []\n",
    "label2 = []\n",
    "names2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip input data files, please make sure the archive.zip is at the same directory as this notebook\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile('./archive.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:17.311554Z",
     "iopub.status.busy": "2022-08-26T21:05:17.310856Z",
     "iopub.status.idle": "2022-08-26T21:05:17.321796Z",
     "shell.execute_reply": "2022-08-26T21:05:17.320789Z",
     "shell.execute_reply.started": "2022-08-26T21:05:17.311518Z"
    }
   },
   "outputs": [],
   "source": [
    "# data preprocessing for running in local environment\n",
    "# might need to change the data_path string according to where the zipped out files are at\n",
    "\n",
    "data_path = './Data'\n",
    "\n",
    "path = data_path + \"/sp14\"\n",
    "try:\n",
    "    for filename in os.listdir(path):\n",
    "        if '.ml' not in filename:\n",
    "            continue\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            s = f.read()\n",
    "            prog, error, fix = proccessing(s)\n",
    "            ins = intersecc(error,fix)\n",
    "            words_,label_ = get_words(prog, ins)\n",
    "            names1.append(filename)\n",
    "            train1.append(words_)\n",
    "            label1.append(label_)\n",
    "            f.close()\n",
    "except:\n",
    "    pass\n",
    "       \n",
    "    \n",
    "path = data_path + \"/fa15\"\n",
    "\n",
    "try:\n",
    "    for filename in os.listdir(path):\n",
    "        if '.ml' not in filename:\n",
    "            continue\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            s = f.read()\n",
    "            prog, error, fix = proccessing(s)\n",
    "            ins = intersecc(error,fix)\n",
    "            words_,label_ = get_words(prog, ins)\n",
    "            names2.append(filename)\n",
    "            train2.append(words_)\n",
    "            label2.append(label_)\n",
    "            f.close()      \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:17.324117Z",
     "iopub.status.busy": "2022-08-26T21:05:17.323450Z",
     "iopub.status.idle": "2022-08-26T21:05:22.378363Z",
     "shell.execute_reply": "2022-08-26T21:05:22.377370Z",
     "shell.execute_reply.started": "2022-08-26T21:05:17.324081Z"
    }
   },
   "outputs": [],
   "source": [
    "# data preprocessing for running on kaggle\n",
    "\n",
    "data_path = '../input/ocamlerrordata/Data'\n",
    "\n",
    "path = data_path + \"/sp14\"\n",
    "try:\n",
    "    for filename in os.listdir(path):\n",
    "        if '.ml' not in filename:\n",
    "            continue\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            s = f.read()\n",
    "            prog, error, fix = proccessing(s)\n",
    "            ins = intersecc(error,fix)\n",
    "            words_,label_ = get_words(prog, ins)\n",
    "            names1.append(filename)\n",
    "            train1.append(words_)\n",
    "            label1.append(label_)\n",
    "            f.close()\n",
    "except:\n",
    "    pass\n",
    "       \n",
    "    \n",
    "path = data_path + \"/fa15\"\n",
    "\n",
    "try:\n",
    "    for filename in os.listdir(path):\n",
    "        if '.ml' not in filename:\n",
    "            continue\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            s = f.read()\n",
    "            prog, error, fix = proccessing(s)\n",
    "            ins = intersecc(error,fix)\n",
    "            words_,label_ = get_words(prog, ins)\n",
    "            names2.append(filename)\n",
    "            train2.append(words_)\n",
    "            label2.append(label_)\n",
    "            f.close()      \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:22.380363Z",
     "iopub.status.busy": "2022-08-26T21:05:22.379989Z",
     "iopub.status.idle": "2022-08-26T21:05:22.390779Z",
     "shell.execute_reply": "2022-08-26T21:05:22.389652Z",
     "shell.execute_reply.started": "2022-08-26T21:05:22.380327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1650'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activate GPU as an accelerator\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "import sklearn\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.__version__\n",
    "\n",
    "MAX_LEN = 250\n",
    "bs = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:22.395814Z",
     "iopub.status.busy": "2022-08-26T21:05:22.395096Z",
     "iopub.status.idle": "2022-08-26T21:05:26.570120Z",
     "shell.execute_reply": "2022-08-26T21:05:26.569156Z",
     "shell.execute_reply.started": "2022-08-26T21:05:22.395776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-small\n",
      "PreTrainedTokenizer(name_or_path='prajjwal1/bert-small', vocab_size=30522, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer based on the model name provided above\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig, RobertaTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-large-cased', do_lower_case=False)\n",
    "\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-small', do_lower_case=False)\n",
    "\n",
    "if model == 'bert-small':\n",
    "    tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-small', do_lower_case=False)\n",
    "elif model == 'bert-base':\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "elif model == 'ocamlbert-large':\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-cased', do_lower_case=False)\n",
    "elif model == 'codebert':\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\", do_lower_case=False)\n",
    "elif model == 'bert-medium':\n",
    "    tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-medium', do_lower_case=False)\n",
    "    \n",
    "print(model)\n",
    "print(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:26.572530Z",
     "iopub.status.busy": "2022-08-26T21:05:26.571632Z",
     "iopub.status.idle": "2022-08-26T21:05:55.544068Z",
     "shell.execute_reply": "2022-08-26T21:05:55.543030Z",
     "shell.execute_reply.started": "2022-08-26T21:05:26.572491Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize words using the tokenizer loaded above\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "\n",
    "tokenized_texts_and_labels1 = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(train1, label1)\n",
    "]\n",
    "\n",
    "tokenized_texts_and_labels2 = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(train2, label2)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenized_texts1 = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels1]\n",
    "labels1 = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels1]\n",
    "\n",
    "tokenized_texts2 = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels2]\n",
    "labels2 = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels2]\n",
    "\n",
    "\n",
    "input_ids1 = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts1],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "input_ids2 = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts2],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "\n",
    "tag_values1 = list(set(label1[0]))\n",
    "tag_values1.append(\"PAD\")\n",
    "tag2idx1 = {t: i for i, t in enumerate(tag_values1)}\n",
    "\n",
    "tags1 = pad_sequences([[tag2idx1.get(l) for l in lab] for lab in labels1],\n",
    "                     maxlen=MAX_LEN, value=tag2idx1[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "\n",
    "\n",
    "tag_values2 = list(set(label2[0]))\n",
    "tag_values2.append(\"PAD\")\n",
    "tag2idx2 = {t: i for i, t in enumerate(tag_values2)}\n",
    "\n",
    "tags2 = pad_sequences([[tag2idx2.get(l) for l in lab] for lab in labels2],\n",
    "                     maxlen=MAX_LEN, value=tag2idx2[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "attention_masks1 = [[float(i != 0.0) for i in ii] for ii in input_ids1]\n",
    "\n",
    "attention_masks2 = [[float(i != 0.0) for i in ii] for ii in input_ids2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:55.546235Z",
     "iopub.status.busy": "2022-08-26T21:05:55.545825Z",
     "iopub.status.idle": "2022-08-26T21:05:55.620796Z",
     "shell.execute_reply": "2022-08-26T21:05:55.619831Z",
     "shell.execute_reply.started": "2022-08-26T21:05:55.546199Z"
    }
   },
   "outputs": [],
   "source": [
    "# divide train and valid datasets based on the task provided before\n",
    "\n",
    "\n",
    "if task == '14to15':\n",
    "    tr_inputs = torch.tensor(input_ids1)\n",
    "    val_inputs = torch.tensor(input_ids2)\n",
    "    tr_tags = torch.tensor(tags1)\n",
    "    val_tags = torch.tensor(tags2)\n",
    "    tr_masks = torch.tensor(attention_masks1)\n",
    "    val_masks = torch.tensor(attention_masks2)\n",
    "elif task == '15to14':\n",
    "    tr_inputs = torch.tensor(input_ids2)\n",
    "    val_inputs = torch.tensor(input_ids1)\n",
    "    tr_tags = torch.tensor(tags2)\n",
    "    val_tags = torch.tensor(tags1)\n",
    "    tr_masks = torch.tensor(attention_masks2)\n",
    "    val_masks = torch.tensor(attention_masks1)\n",
    "\n",
    "# if task == '14to14':\n",
    "#     tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids1, tags1,\n",
    "#                                                             random_state=2018, test_size=0.1)\n",
    "#     tr_masks, val_masks, _, _ = train_test_split(attention_masks1, input_ids1,\n",
    "#                                                  random_state=2018, test_size=0.1)\n",
    "#     tr_inputs = torch.tensor(tr_inputs)\n",
    "#     val_inputs = torch.tensor(val_inputs)\n",
    "#     tr_tags = torch.tensor(tr_tags)\n",
    "#     val_tags = torch.tensor(val_tags)\n",
    "#     tr_masks = torch.tensor(tr_masks)\n",
    "#     val_masks = torch.tensor(val_masks)\n",
    "# elif task == '15to15':\n",
    "#     tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids2, tags2,\n",
    "#                                                             random_state=2018, test_size=0.1)\n",
    "#     tr_masks, val_masks, _, _ = train_test_split(attention_masks2, input_ids2,\n",
    "#                                                  random_state=2018, test_size=0.1)\n",
    "#     tr_inputs = torch.tensor(tr_inputs)\n",
    "#     val_inputs = torch.tensor(val_inputs)\n",
    "#     tr_tags = torch.tensor(tr_tags)\n",
    "#     val_tags = torch.tensor(val_tags)\n",
    "#     tr_masks = torch.tensor(tr_masks)\n",
    "#     val_masks = torch.tensor(val_masks)\n",
    "# elif task == '14to15':\n",
    "#     tr_inputs = torch.tensor(input_ids1)\n",
    "#     val_inputs = torch.tensor(input_ids2)\n",
    "#     tr_tags = torch.tensor(tags1)\n",
    "#     val_tags = torch.tensor(tags2)\n",
    "#     tr_masks = torch.tensor(attention_masks1)\n",
    "#     val_masks = torch.tensor(attention_masks2)\n",
    "# elif task == '15to14':\n",
    "#     tr_inputs = torch.tensor(input_ids2)\n",
    "#     val_inputs = torch.tensor(input_ids1)\n",
    "#     tr_tags = torch.tensor(tags2)\n",
    "#     val_tags = torch.tensor(tags1)\n",
    "#     tr_masks = torch.tensor(attention_masks2)\n",
    "#     val_masks = torch.tensor(attention_masks1)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:55.622810Z",
     "iopub.status.busy": "2022-08-26T21:05:55.622426Z",
     "iopub.status.idle": "2022-08-26T21:05:55.628713Z",
     "shell.execute_reply": "2022-08-26T21:05:55.627473Z",
     "shell.execute_reply.started": "2022-08-26T21:05:55.622769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Task1: sp14 -> sp14\n",
    "# tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids1, tags1,\n",
    "#                                                             random_state=2018, test_size=0.1)\n",
    "# tr_masks, val_masks, _, _ = train_test_split(attention_masks1, input_ids1,\n",
    "#                                              random_state=2018, test_size=0.1)\n",
    "# tr_inputs = torch.tensor(tr_inputs)\n",
    "# val_inputs = torch.tensor(val_inputs)\n",
    "# tr_tags = torch.tensor(tr_tags)\n",
    "# val_tags = torch.tensor(val_tags)\n",
    "# tr_masks = torch.tensor(tr_masks)\n",
    "# val_masks = torch.tensor(val_masks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Task2: fa15 -> fa15\n",
    "# tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids2, tags2,\n",
    "#                                                             random_state=2018, test_size=0.1)\n",
    "# tr_masks, val_masks, _, _ = train_test_split(attention_masks2, input_ids2,\n",
    "#                                              random_state=2018, test_size=0.1)\n",
    "# tr_inputs = torch.tensor(tr_inputs)\n",
    "# val_inputs = torch.tensor(val_inputs)\n",
    "# tr_tags = torch.tensor(tr_tags)\n",
    "# val_tags = torch.tensor(val_tags)\n",
    "# tr_masks = torch.tensor(tr_masks)\n",
    "# val_masks = torch.tensor(val_masks)\n",
    "\n",
    "\n",
    "# Task3: sp14->fa15\n",
    "# tr_inputs = torch.tensor(input_ids1)\n",
    "# val_inputs = torch.tensor(input_ids2)\n",
    "# tr_tags = torch.tensor(tags1)\n",
    "# val_tags = torch.tensor(tags2)\n",
    "# tr_masks = torch.tensor(attention_masks1)\n",
    "# val_masks = torch.tensor(attention_masks2)\n",
    "\n",
    "# Task4: fa15->sp14\n",
    "# tr_inputs = torch.tensor(input_ids2)\n",
    "# val_inputs = torch.tensor(input_ids1)\n",
    "# tr_tags = torch.tensor(tags2)\n",
    "# val_tags = torch.tensor(tags1)\n",
    "# tr_masks = torch.tensor(attention_masks2)\n",
    "# val_masks = torch.tensor(attention_masks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:55.631259Z",
     "iopub.status.busy": "2022-08-26T21:05:55.630585Z",
     "iopub.status.idle": "2022-08-26T21:05:55.643002Z",
     "shell.execute_reply": "2022-08-26T21:05:55.641902Z",
     "shell.execute_reply.started": "2022-08-26T21:05:55.631161Z"
    }
   },
   "outputs": [],
   "source": [
    "# use dataloader to create datasets used for future model training and validating\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:05:55.645180Z",
     "iopub.status.busy": "2022-08-26T21:05:55.644565Z",
     "iopub.status.idle": "2022-08-26T21:06:38.106798Z",
     "shell.execute_reply": "2022-08-26T21:06:38.105751Z",
     "shell.execute_reply.started": "2022-08-26T21:05:55.645140Z"
    }
   },
   "outputs": [],
   "source": [
    "# load pre-trained model based on the model name provided before\n",
    "\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW, BertConfig, RobertaForTokenClassification\n",
    "\n",
    "transformers.__version__\n",
    "\n",
    "if model == 'bert-small' and task[0:2] == '14':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertsmall14',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model =='bert-small' and task[0:2] == '15':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertsmall15',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'bert-base' and task[0:2] == '14':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertbase14',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'bert-base' and task[0:2] == '15':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertbase15',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'codebert' and task[0:2] == '14':\n",
    "    model = RobertaForTokenClassification.from_pretrained(\n",
    "         \"AllenGeng/CodeBert14\",\n",
    "         num_labels=len(tag2idx1),\n",
    "         output_attentions = False,\n",
    "         output_hidden_states = False\n",
    "    )\n",
    "elif model == 'codebert' and task[0:2] == '15':\n",
    "    model = RobertaForTokenClassification.from_pretrained(\n",
    "         \"AllenGeng/CodeBert15\",\n",
    "         num_labels=len(tag2idx1),\n",
    "         output_attentions = False,\n",
    "         output_hidden_states = False\n",
    "    )\n",
    "elif model == 'bert-medium' and task[0:2] == '14':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertmedium14',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'bert-medium' and task[0:2] == '15':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/bertmedium15',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'ocamlbert-large' and task[0:2] == '14':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/ocamlbertlarge14',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n",
    "elif model == 'ocamlbert-large' and task[0:2] == '15':\n",
    "    model = BertForTokenClassification.from_pretrained('TianyuHan/ocamlbertlarge15',num_labels=len(tag2idx1),output_attentions = False,output_hidden_states = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T21:06:38.108697Z",
     "iopub.status.busy": "2022-08-26T21:06:38.108336Z",
     "iopub.status.idle": "2022-08-26T21:06:58.294464Z",
     "shell.execute_reply": "2022-08-26T21:06:58.293376Z",
     "shell.execute_reply.started": "2022-08-26T21:06:38.108662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final result is: 0.52288455\n"
     ]
    }
   ],
   "source": [
    "# model training and results printing\n",
    "\n",
    "model.cuda();\n",
    "# ========================================\n",
    "#               Validation\n",
    "# ========================================\n",
    "# After the completion of each training epoch, measure our performance on\n",
    "# our validation set.\n",
    "\n",
    "# Put the model into evaluation mode\n",
    "model.eval()\n",
    "# Reset the validation loss for this epoch.\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "predictions , true_labels = [], []\n",
    "for batch in valid_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients,\n",
    "    # saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "        b_labels = b_labels.to(torch.int64) # This line is added since running locally with a different GPU requires it. Might need to delete if you are using a different GPU.\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # This will return the logits rather than the loss because we have not provided labels.\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "    # Move logits and labels to CPU\n",
    "    logits = outputs[1].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Calculate the accuracy for this batch of test sentences.\n",
    "    eval_loss += outputs[0].mean().item()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.extend(label_ids)\n",
    "\n",
    "eval_loss = eval_loss / len(valid_dataloader)\n",
    "\n",
    "pred = np.array(predictions)\n",
    "y = np.array(true_labels)\n",
    "pad_mask = np.array(true_labels)\n",
    "pad_mask[pad_mask==0] = 1\n",
    "pad_mask[pad_mask==2] = 0\n",
    "pred = pred & pad_mask\n",
    "y[y==2]=0\n",
    "\n",
    "intersection = pred & y\n",
    "union = pred | y\n",
    "nonzero_intersection = torch.count_nonzero(torch.from_numpy(intersection), dim=1)\n",
    "nonzero_union = torch.count_nonzero(torch.from_numpy(union), dim=1)\n",
    "nonzero_intersection = torch.where(nonzero_union == 0, 1, nonzero_intersection)\n",
    "nonzero_union[nonzero_union==0] = 1\n",
    "\n",
    "\n",
    "acc = nonzero_intersection / nonzero_union\n",
    "\n",
    "print(\"The final result is: \" + str(torch.mean(acc).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def acc_threshold(logits, threshold):\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    prob = []\n",
    "    for i in logits:\n",
    "        logits_ = torch.tensor(i)\n",
    "        prob.append(softmax(logits_))\n",
    "    res = []\n",
    "    for p in prob:\n",
    "        gg = (p[:,1] >= threshold)*1\n",
    "        res.append(gg.tolist())\n",
    "    return res\n",
    "            \n",
    "\n",
    "\n",
    "def eval_(model,threshold):\n",
    "    model.eval()\n",
    "    # Reset the validation loss for this epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            b_labels = b_labels.to(torch.int64)  # This line is added since running locally with a different GPU requires it. Might need to delete if you are using a different GPU.\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "    #    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        predictions.extend(acc_threshold(logits, threshold))\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "#     validation_loss_values.append(eval_loss)\n",
    "#     print(\"Validation loss: {}\".format(eval_loss))\n",
    "\n",
    "    pred = np.array(predictions)\n",
    "    y = np.array(true_labels)\n",
    "    pad_mask = np.array(true_labels)\n",
    "    pad_mask[pad_mask==0] = 1\n",
    "    pad_mask[pad_mask==2] = 0\n",
    "    pred = pred & pad_mask\n",
    "    y[y==2]=0\n",
    "\n",
    "    intersection = pred & y\n",
    "    union = pred | y\n",
    "\n",
    "    nonzero_intersection = torch.count_nonzero(torch.from_numpy(intersection), dim=1)\n",
    "    nonzero_union = torch.count_nonzero(torch.from_numpy(union), dim=1)\n",
    "    nonzero_intersection = torch.where(nonzero_union == 0, 1, nonzero_intersection)\n",
    "    nonzero_union[nonzero_union==0] = 1\n",
    "    acc = nonzero_intersection / nonzero_union\n",
    "    return torch.mean(acc)\n",
    "\n",
    "\n",
    "\n",
    "thel = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "# print(thel)\n",
    "\n",
    "\n",
    "\n",
    "def eval_on_thel(model,thel):\n",
    "    ress = []\n",
    "    for i in thel:\n",
    "        ress.append(eval_(model,i))\n",
    "    return ress\n",
    "accs = eval_on_thel(model,thel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When threshold equals to 0.0, model's diagnosis accuracy evaluates to 19.233%\n",
      "When threshold equals to 0.1, model's diagnosis accuracy evaluates to 53.612%\n",
      "When threshold equals to 0.2, model's diagnosis accuracy evaluates to 54.272%\n",
      "When threshold equals to 0.3, model's diagnosis accuracy evaluates to 53.474%\n",
      "When threshold equals to 0.4, model's diagnosis accuracy evaluates to 52.961%\n",
      "When threshold equals to 0.5, model's diagnosis accuracy evaluates to 52.287%\n",
      "When threshold equals to 0.6, model's diagnosis accuracy evaluates to 51.383%\n",
      "When threshold equals to 0.7, model's diagnosis accuracy evaluates to 49.675%\n",
      "When threshold equals to 0.8, model's diagnosis accuracy evaluates to 48.496%\n",
      "When threshold equals to 0.9, model's diagnosis accuracy evaluates to 44.926%\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"When threshold equals to {}, model's diagnosis accuracy evaluates to {:.3f}%\".format(thel[i],float(accs[i])*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from pickle import FALSE\n",
    "from typing import Sized\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "m_x = []\n",
    "for i in range(10):\n",
    "    m_x.append(i/10)\n",
    "    accs[i] = float(accs[i])\n",
    "\n",
    "plt.plot(m_x,accs, color=\"darkseagreen\", marker=\"o\", label=model_name)\n",
    "plt.plot(m_x,accs, marker = \"o\",  color=\"darkseagreen\")\n",
    "\n",
    "plt.title(\"Impact of type error threshold on {}'s blame accuracy\".format(model_name), pad = 15)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(m_x, m_x)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "plt.grid(False)\n",
    "plt.figure(figsize=(6,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
